{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jirik\\Miniconda3\\envs\\lisa\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import sys\n",
    "import os.path as op\n",
    "import shutil\n",
    "# sys.path.insert(0, \"/home/mjirik/projects/pyseg_base/\")\n",
    "sys.path.insert(0, op.abspath(\"../\"))\n",
    "import scipy\n",
    "import time\n",
    "import pandas as pd\n",
    "import platform\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from imcut import pycut\n",
    "import sed3\n",
    "\n",
    "latex_dir = Path(\"../../papers/cmbbeiv19/tmp/\")\n",
    "\n",
    "\n",
    "fname = \"exp062-multiscale3.csv\"\n",
    "fnamenew = \"msgc_experiment3.csv\"\n",
    "\n",
    "\n",
    "rnd_seed=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Jirik\\\\projects\\\\imcut\\\\examples'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block size bylo 10\n",
    "segparams0 = {\n",
    "    'method':'graphcut',\n",
    "#     'method':'multiscale_graphcut',\n",
    "    'use_boundary_penalties': True,\n",
    "    'boundary_dilatation_distance': 2,\n",
    "    'boundary_penalties_weight': 1,\n",
    "    'block_size': 10,\n",
    "    'tile_zoom_constant': 1\n",
    "    }\n",
    "\n",
    "segparams1 = {\n",
    "    # 'method':'graphcut',\n",
    "    'method':'multiscale_graphcut_hi2lo',\n",
    "    'use_boundary_penalties': True,\n",
    "    'boundary_dilatation_distance': 2,\n",
    "    'boundary_penalties_weight': 1,\n",
    "    'block_size': 10,\n",
    "    'tile_zoom_constant': 1\n",
    "    }\n",
    "\n",
    "segparams2 = {\n",
    "    # 'method':'graphcut',\n",
    "    'method':'multiscale_graphcut_lo2hi',\n",
    "    'use_boundary_penalties': True,\n",
    "    'boundary_dilatation_distance': 2,\n",
    "    'boundary_penalties_weight': 1,\n",
    "    'block_size': 10,\n",
    "    'tile_zoom_constant': 1\n",
    "    }\n",
    "\n",
    "\n",
    "labels = [\n",
    "    \"ssgc \",\n",
    "    \"msgc_hi2lo \",\n",
    "    \"msgc_lo2hi \",\n",
    "]\n",
    "\n",
    "segparamsTri = [segparams0, segparams1, segparams2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(sz=32, offset=0, radius=7, seedsz=3):\n",
    "    #seedsz= int(sz/10)\n",
    "    space=2\n",
    "    rradius = radius / sz\n",
    "#     rseedsz = seedsz / sz\n",
    "    seeds = np.zeros([sz, sz+1, sz+2], dtype=np.int8)\n",
    "    center = [\n",
    "        0.3 + offset, \n",
    "        0.3 + offset, \n",
    "        0.4 + offset\n",
    "    ]\n",
    "    seeds[\n",
    "        int(center[0] * sz), \n",
    "        int(center[1] * sz):int((center[1] + (0.2 * rradius) ) * sz), \n",
    "        int(center[2] * sz):int((center[2] + (0.15 * rradius) ) * sz), \n",
    "    ] = 1\n",
    "    seeds[\n",
    "        int((center[0] + (0.2 * rradius)) * sz), \n",
    "        int((center[1] + (0.1 * rradius)) * sz):int((center[1] + (0.35 * rradius) + (0.03 * seedsz)) * sz), \n",
    "        int((center[2] + (0.02 * rradius)) * sz):int((center[2] + (0.15 * rradius) + (0.03 * seedsz)) * sz), \n",
    "    ] = 1\n",
    "    img = np.ones([sz, sz+1, sz+2])\n",
    "    img = img - seeds\n",
    "\n",
    "\n",
    "    seeds[\n",
    "        int((center[0] - (rradius * 0.5)) * sz):int((center[0] + (rradius * 0.5 ))* sz),\n",
    "#         int(3 + (seedsz * rradius * 0.5 * sz)), \n",
    "        2:int(3 + (seedsz * 0.03 * sz)), \n",
    "        2:int(3 + (seedsz * 0.03 * sz))\n",
    "    ] = 2\n",
    "    img = scipy.ndimage.morphology.distance_transform_edt(img)\n",
    "    segm = img < radius\n",
    "    img = (100 * segm + 80 * np.random.random(img.shape)).astype(np.uint8)\n",
    "    return img, segm, seeds\n",
    "\n",
    "def make_data_old(sz=32, offset=0, radius=7, seedsz=3):\n",
    "    #seedsz= int(sz/10)\n",
    "    space=2\n",
    "    seeds = np.zeros([sz, sz+1, sz+2], dtype=np.int8)\n",
    "    xmin = radius + seedsz + offset + 2\n",
    "    ymin = radius + seedsz + offset + 6\n",
    "    seeds[ offset + 12,  xmin + 3:xmin + 7 + seedsz, ymin:ymin+2] = 1\n",
    "    seeds[ offset + 20, xmin + 7:xmin + 12 + seedsz, ymin+5:ymin+7] = 1\n",
    "    img = np.ones([sz, sz+1, sz+2])\n",
    "    img = img - seeds\n",
    "    seeds[\n",
    "        2:10 + seedsz, \n",
    "        2:9+ seedsz, \n",
    "        2:3+ seedsz] = 2\n",
    "    img = scipy.ndimage.morphology.distance_transform_edt(img)\n",
    "    segm = img < radius\n",
    "    img = (100 * segm + 80 * np.random.random(img.shape)).astype(np.uint8)\n",
    "    return img, segm, seeds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTeX export functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latex_file(df, fn):\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(df.to_latex())\n",
    "        \n",
    "def latex_float(f, precision=4):\n",
    "    float_str = \"{0:.\" + str(int(precision)) + \"g}\"\n",
    "    float_str = float_str.format(f)\n",
    "    if \"e\" in float_str:\n",
    "        base, exponent = float_str.split(\"e\")\n",
    "        return r\"{0} \\times 10^{{{1}}}\".format(base, int(exponent))\n",
    "    else:\n",
    "        return float_str\n",
    "    \n",
    "def float_to_latex_file(fl, fn, precision=4):\n",
    "    string = latex_float(fl, precision=precision)\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(string)\n",
    "\n",
    "def num2latex(num, filename=None, precision=4):\n",
    "    if type(num) is str:\n",
    "        float_str = num\n",
    "    else:\n",
    "        float_str = \"{0:.\" + str(int(precision)) + \"g}\"\n",
    "        float_str = float_str.format(num)\n",
    "        \n",
    "    if float_str[:4] == r\"\\num\":\n",
    "        pass\n",
    "    else:\n",
    "        float_str = \"\\\\num{\" + float_str + \"}\" \n",
    "    if filename is not None:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(float_str)\n",
    "    return float_str\n",
    "\n",
    "def to_file(text, fn):\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better melt\n",
    "from pandas.core.dtypes.common import is_list_like\n",
    "from pandas.core.frame import DataFrame\n",
    "from pandas.core.index import MultiIndex\n",
    "from pandas import compat\n",
    "from IPython.display import display\n",
    "from pandas.core.reshape.concat import concat\n",
    "import re\n",
    "from pandas.core.tools.numeric import to_numeric\n",
    "from pandas.util._decorators import Appender\n",
    "from pandas.core.frame import _shared_docs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.util.testing as tm\n",
    "\n",
    "\n",
    "def _melt(frame, id_vars=None, value_vars=None, var_name=None,\n",
    "          value_name='value', col_level=None, stubnames=False,\n",
    "          suffix=r'\\d+', sep='', extra_group=0, var_end=None):\n",
    "    # TODO: what about the existing index?\n",
    "\n",
    "    def check_vars(frame, var, var_string):\n",
    "        for v in var:\n",
    "            if num_col_levels > 1:\n",
    "                if not isinstance(v, tuple):\n",
    "                    raise ValueError('{} must be a list of tuples'\n",
    "                                     ' when columns are a MultiIndex'\n",
    "                                     .format(var_string))\n",
    "                elif len(v) != num_col_levels:\n",
    "                    raise ValueError('all tuples in {} must be length {}'\n",
    "                                     .format(var_string,\n",
    "                                             frame.columns.nlevels))\n",
    "            else:\n",
    "                if is_list_like(v) and len(v) > 1:\n",
    "                    raise ValueError('DataFrame has only a single level of '\n",
    "                                     'columns. {} is not a column'.format(v))\n",
    "\n",
    "    if len(col_level) == 0:\n",
    "        num_col_levels = frame.columns.nlevels\n",
    "    else:\n",
    "        num_col_levels = len(col_level)\n",
    "\n",
    "    check_vars(frame, id_vars, 'id_vars')\n",
    "    check_vars(frame, value_vars, 'value_vars')\n",
    "\n",
    "    if var_name != [] and len(var_name) != num_col_levels:\n",
    "        raise ValueError('Length of var_name must match effective number of '\n",
    "                         'column levels.')\n",
    "    \n",
    "    if col_level != []:\n",
    "        droplevels = list(range(frame.columns.nlevels))\n",
    "        for level in col_level:\n",
    "            if isinstance(level, int):\n",
    "                droplevels.remove(level)\n",
    "            else:\n",
    "                droplevels.remove(frame.columns.names.index(level))\n",
    "        if droplevels != []:\n",
    "            frame = frame.copy()\n",
    "            frame.columns = frame.columns.droplevel(droplevels)\n",
    "\n",
    "    if stubnames and isinstance(frame.columns, MultiIndex):\n",
    "        raise ValueError('Stubnames only work with single-index DataFrames')\n",
    "        \n",
    "    for iv in id_vars:\n",
    "        if iv not in frame.columns:\n",
    "            raise KeyError('{} not in columns'.format(iv))\n",
    "\n",
    "    if value_vars != []:\n",
    "        for vv in value_vars:\n",
    "            if vv not in frame.columns:\n",
    "                raise KeyError('{} not in columns'.format(vv))\n",
    "                \n",
    "    if var_name == []:\n",
    "        names = list(frame.columns.names)\n",
    "        if len(names) == 1:\n",
    "            if names[0] is None:\n",
    "                var_name.append('variable')\n",
    "            else:\n",
    "                var_name.append(names[0])\n",
    "        elif names.count(None) == 1:\n",
    "            names[names.index(None)] = 'variable'\n",
    "            var_name = names\n",
    "        else:\n",
    "            missing_name_count = 0\n",
    "            for name in names:\n",
    "                if name is None:\n",
    "                    var_name.append('variable_{}'.format(missing_name_count))\n",
    "                    missing_name_count += 1\n",
    "                else:\n",
    "                    var_name.append(name)\n",
    "    if var_end is not None:\n",
    "        var_name = [vn + '_' + str(var_end) for vn in var_name]\n",
    "    \n",
    "    N = len(frame)\n",
    "    \n",
    "    non_id_ilocs = []\n",
    "    if value_vars != []:\n",
    "        for v in value_vars:\n",
    "            for i, v1 in enumerate(frame.columns):\n",
    "                if v == v1:\n",
    "                    non_id_ilocs.append(i)\n",
    "    else:\n",
    "        if id_vars == []:\n",
    "            non_id_ilocs = list(range(frame.shape[1]))\n",
    "        else:\n",
    "            for i, v in enumerate(frame.columns):\n",
    "                if v not in id_vars:\n",
    "                    non_id_ilocs.append(i)\n",
    "                        \n",
    "    K = len(non_id_ilocs)\n",
    "\n",
    "    mdata = {}\n",
    "    mcolumns = []\n",
    "    for col in id_vars:\n",
    "        pandas_obj = frame[col]\n",
    "        if isinstance(pandas_obj, DataFrame):\n",
    "            for i in range(pandas_obj.shape[1]):\n",
    "                col_name = col + '_id_' + str(i)\n",
    "                mdata[col_name] = np.tile(pandas_obj.iloc[:, i].values, K + extra_group)\n",
    "                mcolumns.append(col_name)\n",
    "        else:\n",
    "            mdata[col] = np.tile(pandas_obj, K + extra_group)\n",
    "            mcolumns.append(col)\n",
    "\n",
    "    values = np.concatenate([frame.iloc[:, i] for i in non_id_ilocs])\n",
    "    if extra_group > 0:\n",
    "        values = np.concatenate((values, np.full([N * extra_group], np.nan)))\n",
    "    mdata[value_name[0]] = values\n",
    "    \n",
    "    for i, col in enumerate(var_name):\n",
    "        values = frame.columns[non_id_ilocs]._get_level_values(i)\n",
    "        if stubnames:\n",
    "            regex = '^{0}{1}'.format(re.escape(value_name[0]), re.escape(sep))\n",
    "            values = to_numeric(values.str.replace(regex, ''), errors='ignore')\n",
    "        if isinstance(values, MultiIndex):\n",
    "            # asanyarray will keep the columns as an Index\n",
    "            values = np.asanyarray(values).repeat(N)\n",
    "        else: \n",
    "            data_list = []\n",
    "            for v in values.tolist():\n",
    "                data_list.extend([v] * N)\n",
    "            values = data_list\n",
    "        if extra_group > 0:\n",
    "            values = np.concatenate((values, np.full([N * extra_group], np.nan)))\n",
    "        mdata[col] = values\n",
    "    mcolumns += var_name + value_name\n",
    "    \n",
    "    return mdata, mcolumns\n",
    "\n",
    "\n",
    "@Appender(_shared_docs['melt'] %\n",
    "          dict(caller='pd.melt(df, ',\n",
    "               versionadded=\"\",\n",
    "               other='DataFrame.melt'))\n",
    "def melt(frame, id_vars=None, value_vars=None, var_name=None,\n",
    "         value_name='value', col_level=None, stubnames=False,\n",
    "         suffix=r'\\d+', sep=''):\n",
    "    def convert_to_list(val):\n",
    "        if val is None:\n",
    "            return []\n",
    "        elif not is_list_like(val):\n",
    "            return [val]\n",
    "        else:\n",
    "            return list(val)\n",
    "\n",
    "    def get_var_names(df, stub, sep, suffix):\n",
    "        regex = '^{0}{1}{2}$'.format(re.escape(stub), re.escape(sep), suffix)\n",
    "        col_return = [col for col in df.columns if re.match(regex, col)]\n",
    "        if col_return == []:\n",
    "            raise ValueError('No stubname {}'.format(stub))\n",
    "        return col_return\n",
    "\n",
    "    id_vars = convert_to_list(id_vars)\n",
    "    value_vars = convert_to_list(value_vars)\n",
    "    var_name = convert_to_list(var_name)\n",
    "    value_name = convert_to_list(value_name)\n",
    "    col_level = convert_to_list(col_level)\n",
    "\n",
    "    if stubnames:\n",
    "        if value_vars == []:\n",
    "            raise ValueError('Must provide stubnames as a list to value_vars')\n",
    "        value_name = value_vars\n",
    "        value_vars = [get_var_names(frame, stub, sep, suffix)\n",
    "                      for stub in value_vars]\n",
    "        if var_name == []:\n",
    "            var_name = ['variable_' + v for v in value_name]\n",
    "\n",
    "    if value_vars != [] and isinstance(value_vars[0], list):\n",
    "        if var_name != []:\n",
    "            if len(value_vars) != len(var_name):\n",
    "                raise ValueError('Number of inner lists of value_vars must '\n",
    "                                 'equal length of var_name '\n",
    "                                 '{} != {}'.format(len(value_vars),\n",
    "                                                   len(var_name)))\n",
    "        else:\n",
    "            var_name = [[]] * len(value_vars)\n",
    "\n",
    "        if len(value_name) > 1:\n",
    "            if len(value_vars) != len(value_name):\n",
    "                raise ValueError('Number of inner lists of value_vars must '\n",
    "                                 'equal length of value_name '\n",
    "                                 '{} != {}'.format(len(value_vars),\n",
    "                                                   len(value_name)))\n",
    "        elif not stubnames:\n",
    "            value_name = [value_name[0] + '_' + str(i) for i in range(len(value_vars))]\n",
    "\n",
    "        value_vars_length = []\n",
    "        for vv in value_vars:\n",
    "            count = 0\n",
    "            for col in frame.columns.values:\n",
    "                if col in vv:\n",
    "                    count += 1\n",
    "            value_vars_length.append(count)\n",
    "        max_group_len = max(value_vars_length)  \n",
    "\n",
    "        mdata_list = []\n",
    "        mcolumns_list = []\n",
    "        vars_zipped = zip(value_vars, var_name, value_name, value_vars_length)\n",
    "        for i, (val_v, var_n, val_n, vvl) in enumerate(vars_zipped):\n",
    "            var_n = convert_to_list(var_n)\n",
    "            val_n = convert_to_list(val_n)\n",
    "\n",
    "            id_vars_ = [] if i > 0 else id_vars\n",
    "            var_end = i if var_n == [] else None\n",
    "            \n",
    "            md, mc = _melt(frame, id_vars=id_vars_, value_vars=val_v,\n",
    "                       var_name=var_n, value_name=val_n,\n",
    "                       col_level=col_level, stubnames=stubnames,\n",
    "                       suffix=suffix, sep=sep, \n",
    "                       extra_group=max_group_len - vvl,\n",
    "                       var_end=var_end)\n",
    "\n",
    "            mdata_list.append(md)\n",
    "            mcolumns_list.append(mc)\n",
    "            \n",
    "        mdata = {}\n",
    "        for d in mdata_list:\n",
    "            mdata.update(d)\n",
    "            \n",
    "        mcolumns = [e for lst in mcolumns_list for e in lst]\n",
    "        return DataFrame(mdata, columns=mcolumns)\n",
    "\n",
    "    else:   \n",
    "        mdata, mcolumns =  _melt(frame, id_vars=id_vars, value_vars=value_vars,\n",
    "                             var_name=var_name, value_name=value_name,\n",
    "                             col_level=col_level, stubnames=stubnames,\n",
    "                             suffix=suffix, sep=sep)\n",
    "        return DataFrame(mdata, columns=mcolumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umělá data, opakovaný experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gc_stats(stats1, prefix=None):\n",
    "    if prefix is None:\n",
    "        prefix = \"\"\n",
    "    \n",
    "        \n",
    "    outstats = {}\n",
    "    for key in stats1:\n",
    "        outstats[prefix + key] = stats1[key]\n",
    "        \n",
    "    outstats[prefix + \"nlinks number\"] = np.sum(np.asarray(outstats[prefix + \"nlinks shape\"]), axis=0)[0]\n",
    "    outstats[prefix + \"tlinks number\"] = np.sum(np.asarray(outstats[prefix + \"tlinks shape\"]), axis=0)[0]\n",
    "    outstats.pop(prefix + \"tlinks shape\")\n",
    "    outstats.pop(prefix + \"nlinks shape\")\n",
    "    outstats[prefix + \"edge number\"] = outstats[prefix + \"nlinks number\"] + outstats[prefix + \"tlinks number\"]\n",
    "\n",
    "    return outstats\n",
    "\n",
    "    \n",
    "def merge_stats(stats0, stats1, stats2, labels=None):\n",
    "    if labels is None:\n",
    "        labels = [\"\"] * 3\n",
    "    \n",
    "   \n",
    "    stats0 = process_gc_stats(stats0, labels[0])\n",
    "    stats1 = process_gc_stats(stats1, labels[1])\n",
    "    stats2 = process_gc_stats(stats2, labels[2])\n",
    "    stats = {}\n",
    "    stats.update(stats0)\n",
    "    stats.update(stats1)\n",
    "    stats.update(stats2)\n",
    "\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def run_gc_with_defined_setup(img, segparams, seeds, true_seg, experiment_label=\"\"):\n",
    "    \n",
    "    start = time.time()\n",
    "    gc = pycut.ImageGraphCut(img, segparams=segparams)\n",
    "    gc.set_seeds(seeds)\n",
    "    gc.run()\n",
    "    sg1 = gc.segmentation\n",
    "    stats1 = gc.stats\n",
    "    elapsed1 = (time.time() - start)\n",
    "    err1 = np.sum(np.abs(true_seg - (1 - sg1)))\n",
    "    stats1[\"time\"] = elapsed1\n",
    "    stats1[\"error\"] = err1\n",
    "    stats1[\"experiment\"] = experiment_label\n",
    "    return stats1\n",
    "# def run_gc_with_defined_setup(img, segparams):\n",
    "    \n",
    "#     start = time.time()\n",
    "#     gc = pycut.ImageGraphCut(img, segparams=segparams)\n",
    "#     gc.set_seeds(seeds)\n",
    "#     gc.run()\n",
    "#     sg1 = gc.segmentation\n",
    "#     stats1 = gc.stats\n",
    "#     elapsed1 = (time.time() - start)\n",
    "#     err1 = np.sum(np.abs(seg - (1 - sg1)))\n",
    "#     stats1[\"time\"] = elapsed1\n",
    "#     stats1[\"error\"] = err1\n",
    "#     return stats1\n",
    "\n",
    "def run_gc_3_times(data_params, segparamsTri, experiment_label, i, df, dfnew):\n",
    "    start = time.time()\n",
    "    img, seg, seeds = make_data(data_params[0], data_params[1], data_params[2], data_params[3])\n",
    "    stats0 = run_gc_with_defined_setup(img, segparamsTri[0], seeds, seg, experiment_label=experiment_label)\n",
    "    stats1 = run_gc_with_defined_setup(img, segparamsTri[1], seeds, seg, experiment_label=experiment_label)\n",
    "    stats2 = run_gc_with_defined_setup(img, segparamsTri[2], seeds, seg, experiment_label=experiment_label)\n",
    "    stats = merge_stats(stats0, stats1, stats2, labels)\n",
    "    \n",
    "    stats = add_data_and_algoritm_info(stats, data_params, segparams0, start, seg)\n",
    "    \n",
    "    dfi = pd.DataFrame(stats, index=[i])\n",
    "    \n",
    "    # display(df)\n",
    "    df = df.append(dfi, sort=True)\n",
    "    df.to_csv(fname, index=False)\n",
    "    \n",
    "    dfinew = add_data_seaborn(stats0, data_params, segparamsTri[0], start, seg, i, labels[0])\n",
    "    dfnew = dfnew.append(dfinew, sort=True)\n",
    "    dfinew = add_data_seaborn(stats1, data_params, segparamsTri[1], start, seg, i, labels[1])\n",
    "    dfnew = dfnew.append(dfinew, sort=True)\n",
    "    dfinew = add_data_seaborn(stats2, data_params, segparamsTri[2], start, seg, i, labels[2])\n",
    "    dfnew = dfnew.append(dfinew, sort=True)\n",
    "    \n",
    "    dfnew.to_csv(fnamenew, index=False)\n",
    "    return df, dfnew\n",
    "\n",
    "\n",
    "def add_data_and_algoritm_info(stats, data_params, segparams, start, true_seg):\n",
    "    machine_hostname = platform.node()\n",
    "    #     stats['msgc time'] = elapsed1\n",
    "#     stats['normal time'] = elapsed2\n",
    "    stats['data size'] = data_params[0]\n",
    "    stats['data offset'] = data_params[1]\n",
    "    stats['data radius'] = data_params[2]\n",
    "    stats[\"block size\"] = segparams[\"block_size\"]\n",
    "    stats[\"data seedsz\"] = data_params[3]\n",
    "    stats[\"data image size px\"] = np.prod(true_seg.shape)\n",
    "    stats[\"data object size px\"] = np.sum(true_seg > 0)\n",
    "#     stats[\"GC error\"] = err2\n",
    "#     stats[\"MSGC error\"] = err1\n",
    "    stats['machine hostname'] = machine_hostname\n",
    "    stats['experiment iteration start time'] = start\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def add_data_seaborn(stats, data_params, segparams, start, true_seg, i, label):\n",
    "    stats = process_gc_stats(stats, \"\")\n",
    "    stats = add_data_and_algoritm_info(stats, data_params, segparams, start, true_seg)\n",
    "    stats[\"method\"] = label\n",
    "    dfinew = pd.DataFrame(stats, index=[i*3 + 0])\n",
    "    #dfnew = dfnew.append(dfinew, sort=True)\n",
    "    \n",
    "    return dfinew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "487px",
    "width": "288px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
