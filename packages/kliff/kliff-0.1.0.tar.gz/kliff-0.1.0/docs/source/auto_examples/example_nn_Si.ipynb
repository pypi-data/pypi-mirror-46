{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nTrain a neural network potential\n================================\n\nIn this tutorial, we train a neural network (NN) potential for silicon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to fit the NN potential to a training set of energies and forces\nfrom compressed and stretched diamond silicon structures (the same training set\nused in `tut_kim_sw`).\nDownload the training set :download:`Si_training_set.tar.gz <https://raw.githubusercontent.com/mjwen/kliff/pytorch/examples/Si_training_set.tar.gz>`\nand extract the tarball: ``$ tar xzf Si_training_set.tar.gz``.\nThe data is stored in **extended xyz** format, and see `doc.dataset` for more\ninformation of this format.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>The ``Si_training_set`` is just a toy data set for the purpose to demonstrate\n   how to use KLIFF to train potentials. It should not be used to train any\n   potential for real simulations.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first import the modules that will be used in this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from kliff.descriptors import SymmetryFunction\nfrom kliff.dataset import DataSet\nimport kliff.neuralnetwork as nn\nfrom kliff.loss import Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model\n-----\n\nFor a NN model, we need to specify the descriptor that transforms atomic\nenvironment information to the fingerprints, which the NN modle uses as the input.\nHere, we use the symmetry functions proposed by by Behler and coworkers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "descriptor = SymmetryFunction(cut_name='cos', cut_dists={'Si-Si': 5.0},\n                              hyperparams='set31', normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``cut_name`` and ``cut_dists`` tells the descriptor what type of cutoff\nfunction to use and what the cutoff distances are. ``hyperparams`` specifies the\nthe set of hyperparameters used in the symmetry function descriptor. If you prefer,\nyou can provide a dictionary of your own hyperparameters. And finally,\n``normalize`` informs that the genereated fingerprints should be normalized by\nfirst subtracting the mean and then dividing the standard deviation. This\nnormalization typically makes it easier to optimzie  NN model.\n\nWe can then build the NN model on top of the descriptor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N1 = 10\nN2 = 10\nmodel = nn.NeuralNetwork(descriptor)\nmodel.add_layers(\n    # first hidden layer\n    nn.Linear(descriptor.get_size(), N1),\n    nn.Tanh(),\n    # second hidden layer\n    nn.Linear(N1, N2),\n    nn.Tanh(),\n    # output layer\n    nn.Linear(N2, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above code, we build a NN model with an input layer, two hidden layer, and\nan output layer. The ``descriptor`` carries the information of the input layer, so\nit is not needed to be spcified explicitly. For each hidden layer, we first do a\nlinear transformation using ``nn.Linear(size_in, size_out)`` (essentially carrying\nout $y = xW+b$, where $W$ is the weight matrix of size ``size_in`` by\n``size_out``, and $b$ is a vector of size ``size_out``. Then we apply the\nhyperbolic tangent activation function ``nn.Tanh()`` to the output of the Linear\nlayer (i.e. $y$) so as to add the nonlinearty. We use a Linear layer for the\noutput layer as well, but unlike the hidden layer, no activation function is\napplied here. The input size ``size_in`` of the first hidden layer must be the size\nof the descriptor, which is obtained using ``descriptor.get_size()``. For all other\nlayers (hidden or output), the input size must be equal to the output size of the\nprevious layer. The ``out_size`` of the output layer much be 1 such that the output\nof the NN model is gives the energy of atom.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training set and calculator\n---------------------------\n\nThe training set and the calculator are the same as explaned in `tut_kim_sw`.\nThe only difference is that we need use the\n:mod:`~kliff.neuralnetwork.PytorchANNCalculator()`, which is targeted for the NN\nmodel. Also, its ``create()`` method takes an argument ``reuse`` to inform whether\nto reuse the fingerprints generated from the descriptor if it is present.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# training set\ndataset_name = 'Si_training_set/varying_alat'\ntset = DataSet()\ntset.read(dataset_name)\nconfigs = tset.get_configs()\nprint('Number of configurations:', len(configs))\n\n# calculator\ncalc = nn.PytorchANNCalculator(model)\ncalc.create(configs, reuse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loss function\n-------------\n\nKLIFF uses a loss function to quantify the difference between the training data\nand potential predictions and uses minimization algorithms to reduce the loss as\nmuch as possible.\nIn the following code snippet, we create a loss function that uses the ``Adam``\noptimzier to minimize it. The Adam optimizer supports minimization using\n`mini-batches` of data, and here we use ``100`` configurations in each minimization\nstep (the training set has a total of 400 configuraions as can be seen above), and\nrun through the training set for ``10`` epochs. The learning rate ``lr`` used here\nis ``0.01``, and typically, one may need to play with this to find an acceptable\none that drives the loss down in a reasonable time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# loss\nloss = Loss(calc, residual_data={'forces_weight': 0.3})\nresult = loss.minimize(method='Adam', num_epochs=10, batch_size=100, lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save the trained model to disk, and later can load it back if we want.\nWe can also write the trained model to a KIM model such that it can be used in\nother simulation codes such as LAMMPS via the KIM API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.save('./saved_model.pt')\nmodel.write_kim_model()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}